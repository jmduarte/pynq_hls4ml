{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "We're going to train a fully connected NN with QKeras on the jet tagging dataset and run it on the Pynq.\n",
    "\n",
    "**Note**: Vivado should be on the `$PATH` beforehand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from qkeras.utils import _add_supported_quantized_objects\n",
    "import hls4ml\n",
    "import numpy as np\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "import os\n",
    "os.environ['PATH'] = '/opt/Xilinx/Vivado/2019.2/bin:' + os.environ['PATH']\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.regularizers import l1\n",
    "from qkeras.qlayers import QDense, QActivation\n",
    "from qkeras.quantizers import quantized_bits, quantized_relu\n",
    "from callbacks import all_callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset fetching\n",
    "This is a lot like the hls4ml tutorial, so we will go through quickly.\n",
    "\n",
    "First, we fetch the dataset from OpenML, do the normalization and make a train test split.\n",
    "\n",
    "We save the test dataset to files so that we can use them on the Pynq card later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_openml('hls4ml_lhc_jets_hlf')\n",
    "X, y = data['data'], data['target']\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "y = to_categorical(y, 5)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "scaler = StandardScaler()\n",
    "X_train_val = scaler.fit_transform(X_train_val)\n",
    "X_test = scaler.transform(X_test)\n",
    "np.save('y_test.npy', y_test)\n",
    "np.save('X_test.npy', X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training\n",
    "Our favourite 3 hidden-layer model. 6 bit quantizers everywhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(QDense(64, input_shape=(16,), name='fc1',\n",
    "                 kernel_quantizer=quantized_bits(6,0,alpha=1), bias_quantizer=quantized_bits(6,0,alpha=1),\n",
    "                 kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
    "model.add(QActivation(activation=quantized_relu(6), name='relu1'))\n",
    "model.add(QDense(32, input_shape=(16,), name='fc2',\n",
    "                 kernel_quantizer=quantized_bits(6,0,alpha=1), bias_quantizer=quantized_bits(6,0,alpha=1),\n",
    "                 kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
    "model.add(QActivation(activation=quantized_relu(6), name='relu2'))\n",
    "model.add(QDense(32, input_shape=(16,), name='fc3',\n",
    "                 kernel_quantizer=quantized_bits(6,0,alpha=1), bias_quantizer=quantized_bits(6,0,alpha=1),\n",
    "                 kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
    "model.add(QActivation(activation=quantized_relu(6), name='relu3'))\n",
    "model.add(QDense(5, name='output',\n",
    "                 kernel_quantizer=quantized_bits(6,0,alpha=1), bias_quantizer=quantized_bits(6,0,alpha=1),\n",
    "                 kernel_initializer='lecun_uniform', kernel_regularizer=l1(0.0001)))\n",
    "model.add(Activation(activation='softmax', name='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prune\n",
    "Because why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow_model_optimization.python.core.sparsity.keras import prune, pruning_callbacks, pruning_schedule\n",
    "from tensorflow_model_optimization.sparsity.keras import strip_pruning\n",
    "pruning_params = {\"pruning_schedule\" : pruning_schedule.ConstantSparsity(0.75, begin_step=2000, frequency=100)}\n",
    "model = prune.prune_low_magnitude(model, **pruning_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = not os.path.exists('model_1/KERAS_check_best_model.h5')\n",
    "if train:\n",
    "    adam = Adam(lr=0.0001)\n",
    "    model.compile(optimizer=adam, loss=['categorical_crossentropy'], metrics=['accuracy'])\n",
    "    callbacks = all_callbacks(stop_patience = 1000,\n",
    "                              lr_factor = 0.5,\n",
    "                              lr_patience = 10,\n",
    "                              lr_epsilon = 0.000001,\n",
    "                              lr_cooldown = 2,\n",
    "                              lr_minimum = 0.0000001,\n",
    "                              outputDir = 'model_1')\n",
    "    callbacks.callbacks.append(pruning_callbacks.UpdatePruningStep())\n",
    "    model.fit(X_train_val, y_train_val, batch_size=1024,\n",
    "              epochs=30, validation_split=0.25, shuffle=True,\n",
    "              callbacks = callbacks.callbacks)\n",
    "    # Save the model again but with the pruning 'stripped' to use the regular layer types\n",
    "    model = strip_pruning(model)\n",
    "    model.save('model_1/KERAS_check_best_model.h5')\n",
    "else:\n",
    "    from tensorflow.keras.models import load_model\n",
    "    from qkeras.utils import _add_supported_quantized_objects\n",
    "    co = {}\n",
    "    _add_supported_quantized_objects(co)\n",
    "    model = load_model('model_1/KERAS_check_best_model.h5', custom_objects=co)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotting\n",
    "from sklearn.metrics import accuracy_score\n",
    "y_keras = model.predict(X_test)\n",
    "print(\"Accuracy: {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_keras, axis=1))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make an hls4ml configuration\n",
    "Notice we're using `Strategy: Resource` for every layer, and `ReuseFactor: 64`. The Programmable Logic (FPGA part) of the Pynq SoC is not big compared to VU9P type of parts.\n",
    "\n",
    "We also use some settings which are good for QKeras.\n",
    "\n",
    "Notice the `fpga_part:'xc7z020clg400-1'` and `backend='Pynq'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hls4ml\n",
    "hls4ml.model.optimizer.OutputRoundingSaturationMode.layers = ['Activation']\n",
    "hls4ml.model.optimizer.OutputRoundingSaturationMode.rounding_mode = 'AP_RND'\n",
    "hls4ml.model.optimizer.OutputRoundingSaturationMode.saturation_mode = 'AP_SAT'\n",
    "config = hls4ml.utils.config_from_keras_model(model, granularity='name')\n",
    "config['Model'] = {}\n",
    "config['Model']['ReuseFactor'] = 64\n",
    "config['Model']['Strategy'] = 'Resource'\n",
    "config['Model']['Precision'] = 'ap_fixed<16,6>'\n",
    "config['LayerName']['fc1']['ReuseFactor'] = 64\n",
    "config['LayerName']['fc2']['ReuseFactor'] = 64\n",
    "config['LayerName']['fc3']['ReuseFactor'] = 64\n",
    "config['LayerName']['output']['ReuseFactor'] = 64\n",
    "config['LayerName']['softmax']['exp_table_t'] = 'ap_fixed<18,8>'\n",
    "config['LayerName']['softmax']['inv_table_t'] = 'ap_fixed<18,4>'\n",
    "\n",
    "cfg = hls4ml.converters.create_vivado_config(fpga_part='xc7z020clg400-1')\n",
    "cfg['HLSConfig'] = config\n",
    "cfg['Backend'] = 'Pynq'\n",
    "cfg['KerasModel'] = model\n",
    "cfg['OutputDir'] = 'hls4ml_prj_gui'\n",
    "\n",
    "print(\"-----------------------------------\")\n",
    "plotting.print_dict(cfg)\n",
    "print(\"-----------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert, `predict`, synthesize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls_model = hls4ml.converters.keras_to_hls(cfg)\n",
    "hls_model.compile()\n",
    "y_hls = hls_model.predict(X_test)\n",
    "print(\"Accuracy: {}\".format(accuracy_score(np.argmax(y_hls, axis=1), np.argmax(y_keras, axis=1))))\n",
    "hls_model.build(csim=False,synth=True,export=True)\n",
    "hls4ml.report.read_vivado_report('hls4ml_prj_gui/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bitfile time\n",
    "At this point we can make a bitfile with `hls4ml.templates.PynqBackend.make_bitfile(hls_model)`. For the first run through, let's use the Vivado GUI to get a better idea of what's going on. We'll run the \"board designer\" flow, run Synthesis and Implementation, then check the various reports."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bitfile time 2\n",
    "For the avoidance of any issues due to missteps during the GUI flow, let's make another project & bitfile.\n",
    "\n",
    "The only difference wrt the above is the output directory, and now we run `hls4ml.templates.PynqBackend.make_bitfile(hls_model)` to make the bitfile! This basically executes the `tcl` command for each of the GUI clicks we made before.\n",
    "\n",
    "Check the terminal from where you started the notebook, or from another terminal (e.g. started from the Jupyter notebook) do `tail -f <a log file>` to see some Vivado output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg['OutputDir'] = 'hls4ml_prj'\n",
    "hls_model = hls4ml.converters.keras_to_hls(cfg)\n",
    "hls_model.compile()\n",
    "y_hls = hls_model.predict(X_test)\n",
    "np.save('y_hls.npy', y_hls)\n",
    "print(\"Accuracy: {}\".format(accuracy_score(np.argmax(y_test, axis=1), np.argmax(y_hls, axis=1))))\n",
    "hls_model.build(csim=False,synth=True,export=True)\n",
    "hls4ml.report.read_vivado_report('hls4ml_prj/')\n",
    "hls4ml.templates.PynqBackend.make_bitfile(hls_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PYNQ time\n",
    "Now we can run this on the Pynq board. Set up your Pynq board. You should have the SD card that came with the board. You'll need to connect to your laptop/PC using an ethernet cable for data and USB for power. You may need to change some network settings to be able to connect to the Jupyter notebook.\n",
    "There are more details here: https://pynq.readthedocs.io/en/latest/getting_started/pynq_z2_setup.html\n",
    "\n",
    "Copy (e.g. with `scp` or jupyter notebook upload) these files to the Pynq:\n",
    "- hls4ml_prj/myproject_pynq/project_1.runs/impl_1/design_1_wrapper.bit\n",
    "- hls4ml_prj/myproject_pynq/project_1.srcs/sources_1/bd/design_1/hw_handoff/design_1.hwh\n",
    "\n",
    "For a reason that I don't understand, these files don't seem to be available immediately when Vivado exits, you may have to wait a few minutes... "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
